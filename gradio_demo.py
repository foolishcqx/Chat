#!/usr/bin/env python3
"""
å¯è‰è¯­éŸ³åŠ©æ‰‹ï¼ˆASR + LLM + TTSï¼‰
- å®æ—¶èŠå¤©è®°å½•
- ç»ˆç«¯è€—æ—¶æ‰“å°
- æ—¥å¿—å†™å…¥ chat.log
"""

import os
import json
import time
import datetime
import tempfile
import gradio as gr
import wave
from vosk import Model, KaldiRecognizer, SetLogLevel
from openai import OpenAI
from indextts.infer import IndexTTS
import torchaudio
# -------------------- å…¨å±€å˜é‡ --------------------
chat_history = []      # å‰ç«¯èŠå¤©è®°å½•
log_file   = "chat.log"
SetLogLevel(-1)        # å…³é—­ Vosk å†—ä½™æ—¥å¿—

# æ¨¡å‹åˆå§‹åŒ–
asr_model = Model(r"D:\lasetTTS\code\vosk-model-small-cn-0.22")
tts_model = IndexTTS(model_dir="checkpoints", cfg_path="checkpoints/config.yaml")


# -------------------- å·¥å…·å‡½æ•° --------------------
# å†™å…¥æ—¥å¿—
def write_log(asr_text, reply, t_asr, t_llm, t_tts, audio_duration, duration_ratio):
    total_time = t_asr + t_llm + t_tts  # è®¡ç®—æ€»è€—æ—¶
    with open("index_chat.log", "a", encoding="utf-8") as f:
        f.write("=" * 40 + "\n")
        f.write(f"{datetime.datetime.now():%Y-%m-%d %H:%M:%S}\n")
        f.write(f"ç”¨æˆ·ï¼š{asr_text}\n")
        f.write(f"å¯è‰ï¼š{reply}\n")
        f.write(f"è€—æ—¶ï¼šASR={t_asr:.2f}s | LLM={t_llm:.2f}s | TTS={t_tts:.2f}s\n")
        f.write(f"éŸ³é¢‘æ—¶é•¿ï¼š{audio_duration:.2f}s\n")
        f.write(f"éŸ³é¢‘ç”Ÿæˆæ—¶é—´/éŸ³é¢‘æ—¶é•¿ï¼š{duration_ratio:.2f}\n")
        f.write(f"æ€»è€—æ—¶ï¼š{total_time:.2f}s\n")  # å†™å…¥æ€»è€—æ—¶
# -------------------- ä¸šåŠ¡å‡½æ•° --------------------
# def audio_recognize(wav_path):
#     t0 = time.time()
#     with wave.open(wav_path, "rb") as wf:
#         if wf.getnchannels() != 1 or wf.getsampwidth() != 2:
#             raise ValueError("éŸ³é¢‘å¿…é¡»æ˜¯å•å£°é“ 16-bit PCM WAV")
#         rec = KaldiRecognizer(asr_model, wf.getframerate())
#         rec.SetWords(True)
#         audio_data = wf.readframes(wf.getnframes())
#         rec.AcceptWaveform(audio_data)
#         result = json.loads(rec.FinalResult())
#     text = result.get("text", "").replace(' ', '')
#     print(f"[ASR] {time.time()-t0:.2f}s â†’ {text}")
#     return text, time.time() - t0

def audio_recognize(audio_data, sample_rate):
    t0 = time.time()
    
    # æ£€æŸ¥éŸ³é¢‘æ•°æ®æ˜¯å¦ä¸ºå•å£°é“ 16-bit PCM
    if len(audio_data) % 2 != 0:
        raise ValueError("éŸ³é¢‘æ•°æ®å¿…é¡»æ˜¯ 16-bit PCM æ ¼å¼")
    
    rec = KaldiRecognizer(asr_model, sample_rate)
    rec.SetWords(True)
    
    # å°†äºŒè¿›åˆ¶éŸ³é¢‘æ•°æ®ä¼ é€’ç»™ KaldiRecognizer
    rec.AcceptWaveform(audio_data)
    result = json.loads(rec.FinalResult())
    
    text = result.get("text", "").replace(' ', '')
    print(f"[ASR] {time.time() - t0:.2f}s â†’ {text}")
    return text, time.time() - t0

def llm_chat(content):
    t0 = time.time()
    try:
        client = OpenAI(
            api_key="ms-cb688843-be74-4cdf-8e0b-6237eda42d1e",  # â† æ›¿æ¢
            base_url="https://api-inference.modelscope.cn/v1/"
        )
        response = client.chat.completions.create(
            model="Qwen/Qwen2.5-Coder-32B-Instruct",
            messages=[
                {"role": "system",
                 "content": "å¯è‰æ˜¯è¥¿é£éª‘å£«å›¢çš„ç«èŠ±éª‘å£«ï¼Œæœ€çˆ±ç‚¸é±¼å’Œè¹¦è¹¦ç‚¸å¼¹ï¼è¯´è¯è¦å…ƒæ°”æ»¡æ»¡ï¼Œå¶å°”æåˆ°ç´å›¢é•¿ä¼šæ€•æ€•çš„ã€‚ç°åœ¨å¯è‰è¦ç”¨å¯çˆ±åˆçˆ†ç‚¸çš„æ–¹å¼ä¸æ—…è¡Œè€…äº¤æµå“¦ï¼"},
                {"role": "user", "content": content + "ã€‚å›å¤è¯·é™åˆ¶åœ¨30ä¸ªå­—ä»¥å†…"}
            ],
            stream=False,
            timeout=30
        )
        reply = response.choices[0].message.content.strip()
    except Exception as e:
        reply = f"ç½‘ç»œå¼‚å¸¸ï¼Œå¯è‰æš‚æ—¶ç¦»çº¿ï¼š{e}"
    print(f"[LLM] {time.time()-t0:.2f}s â†’ {reply}")
    return reply, time.time() - t0

def index_tts(text, refer_voice=r"D:\lasetTTS\code\keli\toknow-2.ogg", output_dir=r"D:\lasetTTS\code\temp_audio"):
    t0 = time.time()
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)

    # ç”¨æ—¶é—´æˆ³æˆ– UUID é˜²æ­¢é‡å
    file_name = f"{datetime.datetime.now().strftime('%Y%m%d%H%M%S%f')[:-3]}.wav"
    tmp_wav = os.path.join(output_dir, file_name)
    tts_model.infer(refer_voice, text, output_path=tmp_wav)
    print(f"[TTS] {time.time()-t0:.2f}s â†’ {tmp_wav}")
    return tmp_wav, time.time() - t0

# -------------------- Gradio æµç¨‹ --------------------
def pipeline(audio, history):

    asr_text, t_asr = audio_recognize(audio)
    reply, t_llm = llm_chat(asr_text)
    tts_path, t_tts = index_tts(reply)

    # è®¡ç®—éŸ³é¢‘æ—¶é•¿
    audio_duration = torchaudio.info(tts_path).num_frames / torchaudio.info(tts_path).sample_rate
    duration_ratio = t_tts  / audio_duration if t_tts > 0 else 0

    # è¿½åŠ åˆ°å‰ç«¯èŠå¤©è®°å½•
    history.append([asr_text, reply])
    write_log(asr_text, reply, t_asr, t_llm, t_tts, audio_duration, duration_ratio)

    return history, tts_path

# -------------------- ç•Œé¢ --------------------
with gr.Blocks(title="å¯è‰è¯­éŸ³åŠ©æ‰‹ ğŸ¤ğŸ’¥") as iface:
    gr.Markdown("### ğŸ¤ æŒ‰ä½è¯´è¯ â†’ å®æ—¶è¯†åˆ« â†’ å¯è‰è¯­éŸ³å›å¤")
    with gr.Row():
        with gr.Column(scale=1):
            audio_input = gr.Audio(sources="microphone", type="filepath", label="å½•éŸ³")
            send_btn    = gr.Button("å‘é€", variant="primary")
        with gr.Column(scale=2):
            chatbot     = gr.Chatbot(label="èŠå¤©è®°å½•", height=300)
            audio_output = gr.Audio(label="å¯è‰è¯­éŸ³", autoplay=True)

    send_btn.click(
        pipeline,
        inputs=[audio_input, chatbot],
        outputs=[chatbot, audio_output]
    )

# # å¯åŠ¨
# iface.launch(server_name="127.0.0.1", server_port=7861, share=True)